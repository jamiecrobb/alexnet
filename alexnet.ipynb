{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a3f8fe",
   "metadata": {},
   "source": [
    "## Downloading datasets\n",
    "First we need to download the datasets and load them into batches. To do this we use PyTorch's DataLoader class.\n",
    "\n",
    "The dataset used in this implementation is [PascalVOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/), which is commonly used in object detection problems. This dataset is XML-based.\n",
    "\n",
    "The original AlexNet paper uses ImageNet, a very large dataset of labeled high-res images. PascalVOC, similarly to ImageNet, contains variable resolution images. Therefore it is important to down-scale image resolution due to AlexNet's requirement of constant input dimensionality. \n",
    "\n",
    "`Therefore, we down-sampled the images to a fixed resolution of 256 × 224. Given a\n",
    "rectangular image, we first rescaled the image such that the shorter side was of length 224, and then\n",
    "cropped out the central 224×224 patch from the resulting image. We did not pre-process the images\n",
    "in any other way, except for subtracting the mean activity over the training set from each pixel. So\n",
    "we trained our network on the (centered) raw RGB values of the pixels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5459de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "399acbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeCollate:\n",
    "    def __init__(self, resize_size=(227, 227)):\n",
    "        assert isinstance(resize_size, (int, tuple))\n",
    "        self.resize_size = resize_size\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, labels = zip(*batch)\n",
    "        resized_images = [transforms.functional.resize(img, self.resize_size, antialias=True) for img in images]\n",
    "        batched_images = torch.stack(resized_images)\n",
    "        return batched_images, labels\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(227),\n",
    "    transforms.CenterCrop(227),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4518e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.VOCDetection(\n",
    "    root='./data',\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=ResizeCollate()\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.VOCDetection(\n",
    "    root='./data',\n",
    "    year='2012',\n",
    "    image_set='val',\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=ResizeCollate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fba85763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "# num = 5\n",
    "\n",
    "# def show_batch(batch_images):\n",
    "#     fig,ax = plt.subplots(figsize=(16,12))\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "#     ax.imshow(make_grid(batch_images,nrow=16).permute(1,2,0))\n",
    "\n",
    "# for batch_images, _ in islice(train_loader, num):\n",
    "#     show_batch(batch_images)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "image = batch[0][0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d90a741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=96,\n",
    "                      kernel_size=11,\n",
    "                      stride=4,\n",
    "                      padding=0),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,\n",
    "                         stride=2)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,\n",
    "                         stride=2)\n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=384,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384,\n",
    "                      out_channels=384,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 6 * 6,  \n",
    "                      out_features= 4096),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096,\n",
    "                      out_features=4096),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc3 = nn.Linear(in_features=4096,\n",
    "                             out_features=20)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "\n",
    "        x = x.view(x.size(0), 256*6*6)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bdb477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (l1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (l2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (l3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (l4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (l5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Linear(in_features=4096, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet()\n",
    "\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af2e8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(alexnet.parameters(), momentum=0.9, lr=0.005, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "992687cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 9216]' is invalid for input of size 692224",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      5\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43malexnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/code/personal/alexnet/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/personal/alexnet/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 73\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml4(x)\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml5(x)\n\u001b[0;32m---> 73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 9216]' is invalid for input of size 692224"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "groundtruths = ' '.join(f'{classes[labels[j]]:5s}' for j in range(4))\n",
    "print(f\"GroundTruth: {groundtruths}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
